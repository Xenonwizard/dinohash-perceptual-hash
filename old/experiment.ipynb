{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/singh/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "/home/singh/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is available (SwiGLU)\")\n",
      "/home/singh/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)\n",
      "  warnings.warn(\"xFormers is available (Attention)\")\n",
      "/home/singh/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)\n",
      "  warnings.warn(\"xFormers is available (Block)\")\n"
     ]
    }
   ],
   "source": [
    "dinov2_vitb14_reg = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14_reg').cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cls_token torch.Size([1, 1, 768])\n",
      "pos_embed torch.Size([1, 1370, 768])\n",
      "register_tokens torch.Size([1, 4, 768])\n",
      "mask_token torch.Size([1, 768])\n",
      "patch_embed.proj.weight torch.Size([768, 3, 14, 14])\n",
      "patch_embed.proj.bias torch.Size([768])\n",
      "blocks.0.norm1.weight torch.Size([768])\n",
      "blocks.0.norm1.bias torch.Size([768])\n",
      "blocks.0.attn.qkv.weight torch.Size([2304, 768])\n",
      "blocks.0.attn.qkv.bias torch.Size([2304])\n",
      "blocks.0.attn.proj.weight torch.Size([768, 768])\n",
      "blocks.0.attn.proj.bias torch.Size([768])\n",
      "blocks.0.ls1.gamma torch.Size([768])\n",
      "blocks.0.norm2.weight torch.Size([768])\n",
      "blocks.0.norm2.bias torch.Size([768])\n",
      "blocks.0.mlp.fc1.weight torch.Size([3072, 768])\n",
      "blocks.0.mlp.fc1.bias torch.Size([3072])\n",
      "blocks.0.mlp.fc2.weight torch.Size([768, 3072])\n",
      "blocks.0.mlp.fc2.bias torch.Size([768])\n",
      "blocks.0.ls2.gamma torch.Size([768])\n",
      "blocks.1.norm1.weight torch.Size([768])\n",
      "blocks.1.norm1.bias torch.Size([768])\n",
      "blocks.1.attn.qkv.weight torch.Size([2304, 768])\n",
      "blocks.1.attn.qkv.bias torch.Size([2304])\n",
      "blocks.1.attn.proj.weight torch.Size([768, 768])\n",
      "blocks.1.attn.proj.bias torch.Size([768])\n",
      "blocks.1.ls1.gamma torch.Size([768])\n",
      "blocks.1.norm2.weight torch.Size([768])\n",
      "blocks.1.norm2.bias torch.Size([768])\n",
      "blocks.1.mlp.fc1.weight torch.Size([3072, 768])\n",
      "blocks.1.mlp.fc1.bias torch.Size([3072])\n",
      "blocks.1.mlp.fc2.weight torch.Size([768, 3072])\n",
      "blocks.1.mlp.fc2.bias torch.Size([768])\n",
      "blocks.1.ls2.gamma torch.Size([768])\n",
      "blocks.2.norm1.weight torch.Size([768])\n",
      "blocks.2.norm1.bias torch.Size([768])\n",
      "blocks.2.attn.qkv.weight torch.Size([2304, 768])\n",
      "blocks.2.attn.qkv.bias torch.Size([2304])\n",
      "blocks.2.attn.proj.weight torch.Size([768, 768])\n",
      "blocks.2.attn.proj.bias torch.Size([768])\n",
      "blocks.2.ls1.gamma torch.Size([768])\n",
      "blocks.2.norm2.weight torch.Size([768])\n",
      "blocks.2.norm2.bias torch.Size([768])\n",
      "blocks.2.mlp.fc1.weight torch.Size([3072, 768])\n",
      "blocks.2.mlp.fc1.bias torch.Size([3072])\n",
      "blocks.2.mlp.fc2.weight torch.Size([768, 3072])\n",
      "blocks.2.mlp.fc2.bias torch.Size([768])\n",
      "blocks.2.ls2.gamma torch.Size([768])\n",
      "blocks.3.norm1.weight torch.Size([768])\n",
      "blocks.3.norm1.bias torch.Size([768])\n",
      "blocks.3.attn.qkv.weight torch.Size([2304, 768])\n",
      "blocks.3.attn.qkv.bias torch.Size([2304])\n",
      "blocks.3.attn.proj.weight torch.Size([768, 768])\n",
      "blocks.3.attn.proj.bias torch.Size([768])\n",
      "blocks.3.ls1.gamma torch.Size([768])\n",
      "blocks.3.norm2.weight torch.Size([768])\n",
      "blocks.3.norm2.bias torch.Size([768])\n",
      "blocks.3.mlp.fc1.weight torch.Size([3072, 768])\n",
      "blocks.3.mlp.fc1.bias torch.Size([3072])\n",
      "blocks.3.mlp.fc2.weight torch.Size([768, 3072])\n",
      "blocks.3.mlp.fc2.bias torch.Size([768])\n",
      "blocks.3.ls2.gamma torch.Size([768])\n",
      "blocks.4.norm1.weight torch.Size([768])\n",
      "blocks.4.norm1.bias torch.Size([768])\n",
      "blocks.4.attn.qkv.weight torch.Size([2304, 768])\n",
      "blocks.4.attn.qkv.bias torch.Size([2304])\n",
      "blocks.4.attn.proj.weight torch.Size([768, 768])\n",
      "blocks.4.attn.proj.bias torch.Size([768])\n",
      "blocks.4.ls1.gamma torch.Size([768])\n",
      "blocks.4.norm2.weight torch.Size([768])\n",
      "blocks.4.norm2.bias torch.Size([768])\n",
      "blocks.4.mlp.fc1.weight torch.Size([3072, 768])\n",
      "blocks.4.mlp.fc1.bias torch.Size([3072])\n",
      "blocks.4.mlp.fc2.weight torch.Size([768, 3072])\n",
      "blocks.4.mlp.fc2.bias torch.Size([768])\n",
      "blocks.4.ls2.gamma torch.Size([768])\n",
      "blocks.5.norm1.weight torch.Size([768])\n",
      "blocks.5.norm1.bias torch.Size([768])\n",
      "blocks.5.attn.qkv.weight torch.Size([2304, 768])\n",
      "blocks.5.attn.qkv.bias torch.Size([2304])\n",
      "blocks.5.attn.proj.weight torch.Size([768, 768])\n",
      "blocks.5.attn.proj.bias torch.Size([768])\n",
      "blocks.5.ls1.gamma torch.Size([768])\n",
      "blocks.5.norm2.weight torch.Size([768])\n",
      "blocks.5.norm2.bias torch.Size([768])\n",
      "blocks.5.mlp.fc1.weight torch.Size([3072, 768])\n",
      "blocks.5.mlp.fc1.bias torch.Size([3072])\n",
      "blocks.5.mlp.fc2.weight torch.Size([768, 3072])\n",
      "blocks.5.mlp.fc2.bias torch.Size([768])\n",
      "blocks.5.ls2.gamma torch.Size([768])\n",
      "blocks.6.norm1.weight torch.Size([768])\n",
      "blocks.6.norm1.bias torch.Size([768])\n",
      "blocks.6.attn.qkv.weight torch.Size([2304, 768])\n",
      "blocks.6.attn.qkv.bias torch.Size([2304])\n",
      "blocks.6.attn.proj.weight torch.Size([768, 768])\n",
      "blocks.6.attn.proj.bias torch.Size([768])\n",
      "blocks.6.ls1.gamma torch.Size([768])\n",
      "blocks.6.norm2.weight torch.Size([768])\n",
      "blocks.6.norm2.bias torch.Size([768])\n",
      "blocks.6.mlp.fc1.weight torch.Size([3072, 768])\n",
      "blocks.6.mlp.fc1.bias torch.Size([3072])\n",
      "blocks.6.mlp.fc2.weight torch.Size([768, 3072])\n",
      "blocks.6.mlp.fc2.bias torch.Size([768])\n",
      "blocks.6.ls2.gamma torch.Size([768])\n",
      "blocks.7.norm1.weight torch.Size([768])\n",
      "blocks.7.norm1.bias torch.Size([768])\n",
      "blocks.7.attn.qkv.weight torch.Size([2304, 768])\n",
      "blocks.7.attn.qkv.bias torch.Size([2304])\n",
      "blocks.7.attn.proj.weight torch.Size([768, 768])\n",
      "blocks.7.attn.proj.bias torch.Size([768])\n",
      "blocks.7.ls1.gamma torch.Size([768])\n",
      "blocks.7.norm2.weight torch.Size([768])\n",
      "blocks.7.norm2.bias torch.Size([768])\n",
      "blocks.7.mlp.fc1.weight torch.Size([3072, 768])\n",
      "blocks.7.mlp.fc1.bias torch.Size([3072])\n",
      "blocks.7.mlp.fc2.weight torch.Size([768, 3072])\n",
      "blocks.7.mlp.fc2.bias torch.Size([768])\n",
      "blocks.7.ls2.gamma torch.Size([768])\n",
      "blocks.8.norm1.weight torch.Size([768])\n",
      "blocks.8.norm1.bias torch.Size([768])\n",
      "blocks.8.attn.qkv.weight torch.Size([2304, 768])\n",
      "blocks.8.attn.qkv.bias torch.Size([2304])\n",
      "blocks.8.attn.proj.weight torch.Size([768, 768])\n",
      "blocks.8.attn.proj.bias torch.Size([768])\n",
      "blocks.8.ls1.gamma torch.Size([768])\n",
      "blocks.8.norm2.weight torch.Size([768])\n",
      "blocks.8.norm2.bias torch.Size([768])\n",
      "blocks.8.mlp.fc1.weight torch.Size([3072, 768])\n",
      "blocks.8.mlp.fc1.bias torch.Size([3072])\n",
      "blocks.8.mlp.fc2.weight torch.Size([768, 3072])\n",
      "blocks.8.mlp.fc2.bias torch.Size([768])\n",
      "blocks.8.ls2.gamma torch.Size([768])\n",
      "blocks.9.norm1.weight torch.Size([768])\n",
      "blocks.9.norm1.bias torch.Size([768])\n",
      "blocks.9.attn.qkv.weight torch.Size([2304, 768])\n",
      "blocks.9.attn.qkv.bias torch.Size([2304])\n",
      "blocks.9.attn.proj.weight torch.Size([768, 768])\n",
      "blocks.9.attn.proj.bias torch.Size([768])\n",
      "blocks.9.ls1.gamma torch.Size([768])\n",
      "blocks.9.norm2.weight torch.Size([768])\n",
      "blocks.9.norm2.bias torch.Size([768])\n",
      "blocks.9.mlp.fc1.weight torch.Size([3072, 768])\n",
      "blocks.9.mlp.fc1.bias torch.Size([3072])\n",
      "blocks.9.mlp.fc2.weight torch.Size([768, 3072])\n",
      "blocks.9.mlp.fc2.bias torch.Size([768])\n",
      "blocks.9.ls2.gamma torch.Size([768])\n",
      "blocks.10.norm1.weight torch.Size([768])\n",
      "blocks.10.norm1.bias torch.Size([768])\n",
      "blocks.10.attn.qkv.weight torch.Size([2304, 768])\n",
      "blocks.10.attn.qkv.bias torch.Size([2304])\n",
      "blocks.10.attn.proj.weight torch.Size([768, 768])\n",
      "blocks.10.attn.proj.bias torch.Size([768])\n",
      "blocks.10.ls1.gamma torch.Size([768])\n",
      "blocks.10.norm2.weight torch.Size([768])\n",
      "blocks.10.norm2.bias torch.Size([768])\n",
      "blocks.10.mlp.fc1.weight torch.Size([3072, 768])\n",
      "blocks.10.mlp.fc1.bias torch.Size([3072])\n",
      "blocks.10.mlp.fc2.weight torch.Size([768, 3072])\n",
      "blocks.10.mlp.fc2.bias torch.Size([768])\n",
      "blocks.10.ls2.gamma torch.Size([768])\n",
      "blocks.11.norm1.weight torch.Size([768])\n",
      "blocks.11.norm1.bias torch.Size([768])\n",
      "blocks.11.attn.qkv.weight torch.Size([2304, 768])\n",
      "blocks.11.attn.qkv.bias torch.Size([2304])\n",
      "blocks.11.attn.proj.weight torch.Size([768, 768])\n",
      "blocks.11.attn.proj.bias torch.Size([768])\n",
      "blocks.11.ls1.gamma torch.Size([768])\n",
      "blocks.11.norm2.weight torch.Size([768])\n",
      "blocks.11.norm2.bias torch.Size([768])\n",
      "blocks.11.mlp.fc1.weight torch.Size([3072, 768])\n",
      "blocks.11.mlp.fc1.bias torch.Size([3072])\n",
      "blocks.11.mlp.fc2.weight torch.Size([768, 3072])\n",
      "blocks.11.mlp.fc2.bias torch.Size([768])\n",
      "blocks.11.ls2.gamma torch.Size([768])\n",
      "norm.weight torch.Size([768])\n",
      "norm.bias torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "dinov2_vitb14_reg.eval()\n",
    "for name, param in dinov2_vitb14_reg.named_parameters():\n",
    "    print(name, param.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:32<00:00, 154.82it/s]\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "dataset_folder = './dataset/imagenet/images'\n",
    "image_files = [f for f in os.listdir(dataset_folder)][:5_000]\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "dataset_folder = './path/to/folder'  # Replace with the path to your image folder\n",
    "\n",
    "# Create the ImageFolder dataset\n",
    "dataset = datasets.ImageFolder(dataset_folder, transform=transform)\n",
    "\n",
    "# Create the DataLoader\n",
    "batch_size = 32  # Set the batch size\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 128)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = outputs[:, :128].cpu().numpy()\n",
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FGSM attack (same as before)\n",
    "def fgsm_attack(model, x, y, epsilon):\n",
    "    x.requires_grad = True\n",
    "    output = model(x)\n",
    "    loss = F.cross_entropy(output, y)\n",
    "    loss.backward()\n",
    "    \n",
    "    x_adv = x + epsilon * x.grad.sign()\n",
    "    x_adv = torch.clamp(x_adv, 0, 1)\n",
    "    \n",
    "    return x_adv.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fast Adversarial Finetuning\n",
    "def fast_adversarial_finetuning(model, train_loader, optimizer, epsilon, device):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # Generate adversarial examples\n",
    "        data_adv = fgsm_attack(model, data, target, epsilon)\n",
    "        \n",
    "        # Finetune on clean and adversarial data\n",
    "        optimizer.zero_grad()\n",
    "        output_clean = model(data)\n",
    "        output_adv = model(data_adv)\n",
    "        loss_clean = F.cross_entropy(output_clean, target)\n",
    "        loss_adv = F.cross_entropy(output_adv, target)\n",
    "        loss = 0.5 * (loss_clean + loss_adv)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Batch {batch_idx}, Loss: {loss.item()}')\n",
    "\n",
    "\n",
    "# for param in list(model.parameters())[:-2]:  # Finetune only the last two layers\n",
    "#     param.requires_grad = False\n",
    "\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224),  # ResNet18 expects 224x224 images\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "train_loader = DataLoader(\n",
    "    datasets.CIFAR10('../data', train=True, download=True, transform=transform),\n",
    "    batch_size=64, shuffle=True)\n",
    "\n",
    "epsilon = 0.03  # Maximum perturbation (might need adjustment for your dataset)\n",
    "num_epochs = 5  # Typically fewer epochs for finetuning\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    fast_adversarial_finetuning(model, train_loader, optimizer, epsilon, device)\n",
    "\n",
    "print(\"Finetuning complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proteus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
